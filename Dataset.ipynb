{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:17.662109Z",
     "start_time": "2024-01-14T00:59:17.638835Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model(data, epochs=1):\n",
    "    class_names = 525\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(class_names)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(data, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:17.663681Z",
     "start_time": "2024-01-14T00:59:17.653507Z"
    }
   },
   "id": "f3bfa7367421db2c",
   "execution_count": 233
  },
  {
   "cell_type": "markdown",
   "source": [
    "Аналог tf.keras.utils.image_dataset_from_directory. Реализация должна поддерживать следующие аргументы: directory, batch_size, image_size, shuffle, seed, validation_split, subset; и возвращать объект класса tf.data.Dataset. Элементом датасета является пара батч изображений и их классов. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f4c5fb41dd129d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_labels(directory):\n",
    "    image_paths, labels = [], []\n",
    "    class_names = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    for label in class_names:\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(class_dir, img_file))\n",
    "                labels.append(label)\n",
    "    return image_paths, labels, class_names\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:17.691519Z",
     "start_time": "2024-01-14T00:59:17.667164Z"
    }
   },
   "id": "c23b7608921a93aa",
   "execution_count": 234
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_image(file_path, image_size):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b3024b9c333d0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomContrast(factor=0.25),\n",
    "    tf.keras.layers.RandomBrightness(factor=0.3),\n",
    "    tf.keras.layers.RandomRotation(factor=0.15),\n",
    "    tf.keras.layers.GaussianNoise(stddev=0.1),\n",
    "    tf.keras.layers.Rescaling(scale=1. / 255)\n",
    "])\n",
    "\n",
    "def preprocess_image_aug(file_path, image_size):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = augmentation(image)\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T01:55:01.230141Z",
     "start_time": "2024-01-14T01:55:01.221742Z"
    }
   },
   "id": "1a8785a80f261b06",
   "execution_count": 250
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def image_dataset_from_directory(directory, batch_size=32, image_size=(256, 256), shuffle=True, seed=None,\n",
    "                                 validation_split=None, subset=None, preprocessing_fun=preprocess_image):\n",
    "    image_paths, labels, class_names = load_labels(directory)\n",
    "    class_indices = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "    labels = [class_indices[label] for label in labels]\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    if validation_split:\n",
    "        train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "            image_paths, labels, test_size=validation_split, random_state=seed, shuffle = shuffle)\n",
    "\n",
    "        if subset == 'training':\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "        elif subset == 'validation':\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "        elif subset == 'both':\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "            train_dataset = train_dataset.map(lambda x, y: (preprocessing_fun(x, image_size), y))\n",
    "            val_dataset = val_dataset.map(lambda x, y: (preprocessing_fun(x, image_size), y))\n",
    "\n",
    "            if shuffle:\n",
    "                train_dataset = train_dataset.shuffle(buffer_size=1000, seed=seed)\n",
    "\n",
    "            train_dataset_batched = train_dataset.batch(batch_size)\n",
    "            val_dataset_batched = val_dataset.batch(batch_size)\n",
    "            return train_dataset_batched, val_dataset_batched\n",
    "        else:\n",
    "            raise ValueError(\"Value should be 'training', 'validation' or 'both\")\n",
    "        dataset = dataset.map(lambda x, y: (preprocessing_fun(x, image_size), y))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(lambda x, y: (preprocessing_fun(x, image_size), y))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000, seed=seed)\n",
    "\n",
    "    dataset_batched = dataset.batch(batch_size)\n",
    "    return dataset_batched"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T01:55:01.693428Z",
     "start_time": "2024-01-14T01:55:01.677451Z"
    }
   },
   "id": "bf052a662747dda4",
   "execution_count": 251
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 5s 112ms/step - loss: 47.6082 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/merkost/Downloads/archive/test'\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "shuffle = True\n",
    "seed = 42\n",
    "validation_split = 0.2\n",
    "subset = 'validation'\n",
    "\n",
    "train_dataset = image_dataset_from_directory(directory, batch_size=batch_size, image_size=image_size, shuffle=shuffle,\n",
    "                                             seed=seed, validation_split=validation_split, subset=subset, preprocessing_fun=preprocess_image_aug)\n",
    "test_model(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T01:55:07.726634Z",
     "start_time": "2024-01-14T01:55:02.596681Z"
    }
   },
   "id": "ff599a4b1b8503f0",
   "execution_count": 252
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функцию\n",
    "\n",
    "def load_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4e4775a66e7c6d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_dataset(path, batch_size, image_size, shuffle, split, preprocessing_fun=preprocess_image):\n",
    "    \"\"\"Given a `path` to a csv index file loads one of the dataset splits. Paths in the index are assumed to be relative to the csv file. The file contains three columns: \"filepaths\", \"labels\" and \"data set\", path to the image, image label and dataset split respectively.\n",
    "\n",
    "    Arguments:\n",
    "        path: path to the csv index file\n",
    "        batch_size: size of batches in the dataset\n",
    "        image_size: size to resize the images to\n",
    "        shuffle: whether to shuffle the index. If False original index order is preserved\n",
    "        split: split to use. One of \"train\", \"valid\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        The loaded dataset\n",
    "        A dictionary mapping class indices to class names\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df_split = df[df['data set'] == split].copy()\n",
    "\n",
    "    class_names = df_split['labels'].unique()\n",
    "    class_dict = {name: i for i, name in enumerate(class_names)}\n",
    "    df_split['labels'] = df_split['labels'].map(class_dict)\n",
    "    base_dir = os.path.dirname(path)\n",
    "    df_split['filepaths'] = df_split['filepaths'].apply(lambda x: os.path.join(base_dir, x))\n",
    "    paths = df_split['filepaths'].values\n",
    "    labels = df_split['labels'].values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: (preprocessing_fun(x, image_size), y))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(paths))\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset, class_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:20.185092Z",
     "start_time": "2024-01-14T00:59:20.176493Z"
    }
   },
   "id": "bd1c8faa049416de",
   "execution_count": 238
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 8s 97ms/step - loss: 6243.2969 - accuracy: 3.8095e-04\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/merkost/Downloads/archive/birds.csv'\n",
    "ds, clss = load_dataset(path, batch_size, image_size, False, split='test')\n",
    "\n",
    "test_model(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:28.926828Z",
     "start_time": "2024-01-14T00:59:20.182331Z"
    }
   },
   "id": "d03c1ff6e57047b4",
   "execution_count": 239
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LoadDatasetSequence(Sequence):\n",
    "    def __init__(self, path, batch_size, image_size, shuffle, split, preprocessing_fun=preprocess_image):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.preprocessing_fun = preprocessing_fun\n",
    "        self.df = pd.read_csv(path)\n",
    "        print(self.df.head())\n",
    "        self.df = self.df[self.df['data set'] == split]\n",
    "        self.class_names = self.df['labels'].unique()\n",
    "        self.class_dict = {name: i for i, name in enumerate(self.class_names)}\n",
    "        self.df['labels'] = self.df['labels'].map(self.class_dict)\n",
    "        base_dir = os.path.dirname(path)\n",
    "        self.df['filepaths'] = self.df['filepaths'].apply(lambda x: os.path.join(base_dir, x))\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1, ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.df) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_df = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        labels = batch_df['labels'].to_numpy()\n",
    "        images = np.array([self.preprocessing_fun(path, self.image_size) for path in batch_df['filepaths']])\n",
    "        return images, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:28.927334Z",
     "start_time": "2024-01-14T00:59:28.919493Z"
    }
   },
   "id": "a1ca4b2a9e20b2d1",
   "execution_count": 240
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class id                      filepaths           labels data set  \\\n",
      "0       0.0  train/ABBOTTS BABBLER/001.jpg  ABBOTTS BABBLER    train   \n",
      "1       0.0  train/ABBOTTS BABBLER/007.jpg  ABBOTTS BABBLER    train   \n",
      "2       0.0  train/ABBOTTS BABBLER/008.jpg  ABBOTTS BABBLER    train   \n",
      "3       0.0  train/ABBOTTS BABBLER/009.jpg  ABBOTTS BABBLER    train   \n",
      "4       0.0  train/ABBOTTS BABBLER/002.jpg  ABBOTTS BABBLER    train   \n",
      "\n",
      "        scientific name  \n",
      "0  MALACOCINCLA ABBOTTI  \n",
      "1  MALACOCINCLA ABBOTTI  \n",
      "2  MALACOCINCLA ABBOTTI  \n",
      "3  MALACOCINCLA ABBOTTI  \n",
      "4  MALACOCINCLA ABBOTTI  \n",
      "83/83 [==============================] - 18s 217ms/step - loss: 3509.1841 - accuracy: 0.0015\n"
     ]
    }
   ],
   "source": [
    "path_to_csv = '/Users/merkost/Downloads/archive/birds.csv'\n",
    "\n",
    "load_seq = LoadDatasetSequence(path_to_csv, batch_size, image_size, shuffle, split='test')\n",
    "\n",
    "test_model(load_seq)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:47.559254Z",
     "start_time": "2024-01-14T00:59:28.922399Z"
    }
   },
   "id": "52fed1c93e240345",
   "execution_count": 241
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ImageDataGenerator(Sequence):\n",
    "    def __init__(self, directory, batch_size=32, image_size=(224,224), shuffle=True, seed=None, validation_split=None, subset=None, preprocessing_fun=preprocess_image):\n",
    "        self.image_paths, self.labels, self.class_names = load_labels(directory)\n",
    "        self.image_paths = np.array(self.image_paths)\n",
    "        self.class_indices = {class_name: i for i, class_name in enumerate(self.class_names)}\n",
    "        self.labels = [self.class_indices[label] for label in self.labels]\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.preprocessing_fun = preprocessing_fun\n",
    "        self.total_count = len(self.image_paths)\n",
    "        print('Total images: ', self.total_count)\n",
    "\n",
    "        if validation_split:\n",
    "            self.train_paths, self.val_paths, self.train_labels, self.val_labels = train_test_split(\n",
    "                self.image_paths, self.labels, test_size=validation_split, random_state=seed)\n",
    "\n",
    "            if subset == 'training':\n",
    "                self.image_paths = self.train_paths\n",
    "                self.labels = self.train_labels\n",
    "            elif subset == 'validation':\n",
    "                self.image_paths = self.val_paths\n",
    "                self.labels = self.val_labels\n",
    "            else:\n",
    "                # self.image_paths = self.train_paths + self.val_paths\n",
    "                # self.labels = self.train_labels + self.val_labels\n",
    "                # if shuffle:\n",
    "                #     shuffle(self)\n",
    "                pass\n",
    "            \n",
    "        if self.shuffle:\n",
    "            self.__shuffle__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.image_paths) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "        paths = self.image_paths[start:end]\n",
    "        labels = self.labels[start:end]\n",
    "        images = np.array([self.preprocessing_fun(path, self.image_size) for path in paths])\n",
    "        return images, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.__shuffle__()\n",
    "            \n",
    "    def __shuffle__(self):\n",
    "        indices = np.arange(len(self.image_paths))\n",
    "        np.random.shuffle(indices)\n",
    "        self.image_paths = self.image_paths[indices]\n",
    "        self.labels = self.labels[indices]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T00:59:47.559657Z",
     "start_time": "2024-01-14T00:59:47.554166Z"
    }
   },
   "id": "51f09f93cfb53919",
   "execution_count": 242
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  2625\n",
      "83/83 [==============================] - 19s 223ms/step - loss: 5360.3003 - accuracy: 0.0011\n"
     ]
    }
   ],
   "source": [
    "from_dir_seq = ImageDataGenerator(directory, subset='train')\n",
    "test_model(from_dir_seq)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T01:00:06.733541Z",
     "start_time": "2024-01-14T00:59:47.557305Z"
    }
   },
   "id": "ac3128b8f0ea342f",
   "execution_count": 243
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
